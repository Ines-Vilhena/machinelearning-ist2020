# -*- coding: utf-8 -*-
"""MLEARNING EXAM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n5qaxnB50DXIg9dAUm_broAbJsYG1nxn

[Understand NumPy np.multiply(), np.dot() and * Operation: A Beginners Guide](https://www.tutorialexample.com/understand-numpy-np-multiply-np-dot-and-operation-a-beginner-guide-numpy-tutorial/)
"""

import numpy as np
from scipy.stats import norm
from scipy import spatial
import math

"""### PB nº 1 - Basic perceptron and DTs

####Basic perceptron
"""

#math.log?

#because np.sign outputs 0 if x is 0

def personalized_sign_func(x):
  if x>= 0:
    return 1
  else:
    return -1

#PB nº 1, ex.1

per_inputs = np.array([ [1,0,0,0], [1,0,2,1], [1,1,1,1], [1,1,-1,0] ])
per_targets = np.array([-1, 1, 1, -1])

per_weights = [1, 1, 1, 1]
per_learningrate = 1

per_outputs = [0, 0, 0, 0]

for r in range(len(per_inputs)):
  per_outputs[r] = personalized_sign_func(np.dot(per_inputs[r],per_weights))
  print("\noutput: " + str(per_outputs[r]))

  if per_outputs[r] != per_targets[r]:
    print("target was " + str(per_targets[r]) + " so the output is wrong!")
    per_weights = per_weights + per_learningrate * (per_targets[r] - per_outputs[r]) * per_inputs[r]
    print("new weights:")
    print(per_weights)
  else:
    print("correct output!")

"""####DT: ID3, entropy, info gain"""

#math.log(number, baseoflog)

def entropy(*probs):
  result = 0
  for prob in probs:
    if prob == 0:
      return 0
    result += prob * math.log(prob, 2)
  
  if result != 0:
    return result * (-1)

#tests
#print(entropy(0.5, 0.5))
#print(entropy(0.99, 0.01))
#print(entropy(0.998, 0.001, 0.001))
#print(entropy(1, 0))
print(entropy(1/5, 1/5, 1/5, 2/5))

#check initial probabilities

#~~~~~~~~~~~~~~~~~~~~~~~~~~
#existence_counter = 0
#for i in range(len(id3_varF1)):
#  if id3_attrF1[i] == 'a' and id3_attrF2[i] == 'a' and id3_attrF3[i] == 'a' and id3_class[i] == '+':
#    print("it exists, of that class exactly!")
#    existence_counter += 1
#print(existence_counter)
#~~~~~~~~~~~~~~~~~~~~~~~~~~


#initial data

id3_attrF1 = np.array(['c', 'a', 'a', 'c', 'a'])
id3_attrF2 = np.array(['a', 'a', 'b', 'b', 'b'])
id3_attrF3 = np.array(['b', 'c', 'b', 'c', 'b'])
id3_attrF4 = np.array(['x', 'a', 'a', 'x', 'a'])

id3_class = np.array (['n', 't', 't', 'm', 'f'])



#check probability of 'a', 'b', 'c'...

var_counter = 0
var_wanted = 'a'
attr_wanted = 'F2'
res = 0

for var in id3_attrF2:
  if var == var_wanted:
    var_counter +=1
res = (var_counter / len(id3_attrF2))
                        
print("prob of " + var_wanted + " in " + attr_wanted + " is: {}".format(res))
print()

#check probabilities of each class

#var_counter = 0
#var_wanted = 't'
#attr_wanted = 'class'
#res = 0

#for var in id3_class:
#  if var == var_wanted:
#    var_counter +=1
#res = (var_counter / len(id3_class))
#print("prob of " + var_wanted + " in " + attr_wanted + " is: {}".format(res))


#check probabilities of each class

class_list = ['n', 't', 'm', 'f']
class_probs = [0,0,0,0]
print("existem {} classes".format(len(id3_class)))
print()

aux_index = 0
for var in class_list:
  tentando = np.where(id3_class == var)
  print("a classe '" + var + "' ocorre " + str(len(tentando[0])) + " vezes no dataset")
  class_probs[aux_index] = len(tentando[0])/len(id3_class)
  aux_index+=1
print("probabilities:")
print(class_probs)



#check if a certain sample exists, and if so, which class it is

for i in range(len(id3_attrF1)):
  if id3_attrF1[i] == 'a' and id3_attrF2[i] == 'b' and id3_attrF3[i] == 'c':
    #print("\nit exists! and the class is: {}".format(id3_class[i]))
    continue

#calculate entropy of start
#***** IMP *****
#DON'T FORGET TO UPDATE ENTROPY_START WHEN CHOOSING A NEW ROOT
entropy_start = entropy(*class_probs)
print("entropy start: " + str(entropy_start))
print()



#calculate entropy of root being F1, F2, F3..

#1º - por aqui os simbolos que existem nos atributos
var_list = ['a', 'b','c', 'x']
#2º mudar aqui qual dos atributos estou a analisar
attr_wanted = 'F3'

for letra in var_list:

  for i in range(len(id3_attrF1)):  
    if id3_attrF3[i] == letra: #<--- 3º mudar o atributo aqui tambem! attrF1, attrF2..
      print(attr_wanted+"="+letra + " exists with class: {}".format(id3_class[i]))
  print()
    
entr=0
print("final entropy of " + attr_wanted + " (mean of individual entropies):")
#4º por a linha de baixo em comentario para nao dar uma coisa estranha
entr=(3/5)*entropy(1/3, 1/3, 1/3) + (2/5)*entropy(0.5, 0.5)
#5º editar a linha de cima com os resultados que deu, tirar o # e tcharan
print(entr)
print("info gain of " + attr_wanted + " (entropy start - entropy of this attr):")
print(entropy_start - entr)

"""####DT: CART, Gini"""

def gini_calc(*probs):
  somatorio = 0
  for prob in probs:
    somatorio += prob**2
  
  return 1 - somatorio

#tests
print(gini_calc(0.5, 0.5))
print(gini_calc(3/4, 1/4))
print(gini_calc(2/3, 1/3))

#check initial probabilities

#~~~~~~~~~~~~~~~~~~~~~~~~~~
#existence_counter = 0
#for i in range(len(id3_varF1)):
#  if id3_attrF1[i] == 'a' and id3_attrF2[i] == 'a' and id3_attrF3[i] == 'a' and id3_class[i] == '+':
#    print("it exists, of that class exactly!")
#    existence_counter += 1
#print(existence_counter)
#~~~~~~~~~~~~~~~~~~~~~~~~~~


#initial data

id3_attrF1 = np.array(['a', 'c', 'c', 'b', 'a', 'b'])
id3_attrF2 = np.array(['a', 'b', 'a', 'a', 'b', 'b'])
id3_attrF3 = np.array(['a', 'c', 'c', 'a', 'c', 'c'])

id3_class = np.array (['+', '+', '+', '-', '-', '-'])




#check probabilities of '-' and '+'
var_counter = 0
var_wanted = '-'
attr_wanted = 'class'
res = 0

for var in id3_class:
  if var == var_wanted:
    var_counter +=1
res = (var_counter / len(id3_class))
#print("prob of " + var_wanted + " in " + attr_wanted + " is: {}".format(res))



#check if a certain sample exists, and if so, which class it is

for i in range(len(id3_varF1)):
  if id3_attrF1[i] == 'a' and id3_attrF2[i] == 'b' and id3_attrF3[i] == 'c':
    #print("\nit exists! and the class is: {}".format(id3_class[i]))
    continue

#calculate gini start, probability of the class being '+' and of being '-' in current dataset
gini_start = gini_calc(0.5, 0.5)
print("gini start: " + str(gini_start))
print()


#check probability of var_wanted in attr_wanted, and probability of remaining split

var_counter_is = 0
var_counter_isnot = 0
var_wanted = 'a'
attr_wanted = 'F3'
res = 0

for var in id3_attrF3:
  
  if var == var_wanted:
    var_counter_is +=1
  res_is = (var_counter_is / len(id3_attrF2))

  if var != var_wanted:
    var_counter_isnot +=1
  res_isnot = (var_counter_isnot / len(id3_attrF2))

                        
print("prob of " + var_wanted + " in " + attr_wanted + " is: {}".format(res_is))
print("prob of NOT " + var_wanted + " in " + attr_wanted + " is: {}".format(res_isnot))
print()



#calculate classes for var_wanted and for remaining split

for i in range(len(id3_varF1)):  

    if id3_attrF2[i] == var_wanted: #<--- change attribute here, F1, F2 or F3
      print(attr_wanted+"="+var_wanted + " exists with the class: {}".format(id3_class[i]))
    else:
      print(attr_wanted+"!="+var_wanted + " exists with the class: {}".format(id3_class[i]))
print()
    


print("gini de " + var_wanted + " vs ~" + var_wanted + " em " + attr_wanted + " (média das entropias das vars individuais):")
entr = res_is*gini_calc(2/3, 1/3) + res_isnot*gini_calc(2/3, 1/3)
print(entr)
print("delta gini de " + attr_wanted + " (gini start - gini do attr):")
print(gini_start - entr)

"""###PB nº 2 - Probability and Gaussians"""

###Testes

a = np.array([[1,2],[3,4]])
b = np.array([[5,6],[7,8]])
print(np.dot(a,b)) #normal multiplication
print()
print(a*b) #element wise multiplication

"""####Univariate"""

#UNIVARIATE
#PB #2, ex. 2

input_data = [180,160,200,171,159,150]
mean_value = np.mean(input_data)
print("mean value: " + str(mean_value) + "\n")

std_dev_squared = np.var(input_data, ddof=1)
std_dev = std_dev_squared**(0.5)
print("standard deviation (already square root): " + str(std_dev) + "\n")

#result_distrib = norm.pdf(100, loc=mean_value, scale=std_dev)
#print("distribution for x1: " + str(result_distrib))

"""####Multivariate"""

#testes

#abc = np.array([[ 5, 1 ,3], 
#                [ 1, 1 ,1], 
#                [ 1, 2 ,1]])

#bcd = np.array([1, 2, 3])

#print (np.dot(abc,bcd))

#PB #2, ex. 3

data_x1 = [2, 1, 0, 2]
data_x2 = [-2, 3, -1, 1]

multi_mean_value = np.mean([data_x1, data_x2], axis=1)
print("multivariate mean value: " + str(multi_mean_value))

print("\nCovariance matrix:")
cov_mat = np.cov(data_x1, data_x2)
print(cov_mat)

cov_mat_determ = np.linalg.det(cov_mat)

print("Cov mat determinant: " + str(cov_mat_determ) + "\n")

inv_final = np.linalg.inv(cov_mat)
print(inv_final)

"""####1-D Gaussian"""

print("For Class C=0 \n")

#Inputs
x1_1d_gauss = [170, 60, 50]
x2_1d_gauss = [160, 160, 150]

# --- For variable X1 ---

x1_mean_value_1d_gauss = np.mean(x1_1d_gauss)
print("mean value x1: " + str(x1_mean_value_1d_gauss))

x1_squared_std_dev = np.var(x1_1d_gauss, ddof=1)
x1_std_dev = x1_squared_std_dev**(0.5)
print("standard deviation x1 (already square root): " + str(x1_std_dev) + "\n")

# --- For variable X2 ---

x2_mean_value_1d_gauss = np.mean(x2_1d_gauss)
print("mean value x2: " + str(x2_mean_value_1d_gauss))

x2_squared_std_dev = np.var(x2_1d_gauss, ddof=1)
x2_std_dev = x2_squared_std_dev**(0.5)
print("standard deviation x2: " + str(x2_std_dev) + "\n")

#Distributions

x1_distrib = norm.pdf(100, loc=x1_mean_value_1d_gauss, scale=x1_std_dev)
print("distribution for x1: " + str(x1_distrib))

x2_distrib = norm.pdf(225, loc=x2_mean_value_1d_gauss, scale=x2_std_dev)
print("distribution for x1: " + str(x2_distrib) + "\n")

#Final calculations

prob_of_class = 0.5
upper_fraction_final = prob_of_class * x1_distrib * x2_distrib

print ("Final result of upper fraction (last two results multiplied): " + str(upper_fraction_final))

print("For Class C=1 \n")

#Inputs
x1_1d_gauss = [80,90,70]
x2_1d_gauss = [220,200,190]

# --- For variable X1 ---

x1_mean_value_1d_gauss = np.mean(x1_1d_gauss)
print("mean value x1: " + str(x1_mean_value_1d_gauss))

x1_squared_std_dev = np.var(x1_1d_gauss, ddof=1)
x1_std_dev = x1_squared_std_dev**(0.5)
print("standard deviation x1 (already square root): " + str(x1_std_dev) + "\n")

# --- For variable X2 ---

x2_mean_value_1d_gauss = np.mean(x2_1d_gauss)
print("mean value x2: " + str(x2_mean_value_1d_gauss))

x2_squared_std_dev = np.var(x2_1d_gauss, ddof=1)
x2_std_dev = x2_squared_std_dev**(0.5)
print("standard deviation x2: " + str(x2_std_dev) + "\n")

#Distributions

x1_distrib = norm.pdf(100, loc=x1_mean_value_1d_gauss, scale=x1_std_dev)
print("distribution for x1: " + str(x1_distrib))

x2_distrib = norm.pdf(225, loc=x2_mean_value_1d_gauss, scale=x2_std_dev)
print("distribution for x1: " + str(x2_distrib) + "\n")

#Final calculations

prob_of_class = 0.5
upper_fraction_final = prob_of_class * x1_distrib * x2_distrib

print ("Final result of upper fraction: " + str(upper_fraction_final))

"""#### 2-D Gaussian"""

print("For Class C=0\n")

x1_2d_gauss = [170, 60, 50]
x2_2d_gauss = [160, 160, 150]

mean_value_2d_gausss = np.mean([x1_2d_gauss, x2_2d_gauss], axis=1)
print("mean value for 2D Gaussian: " + str(mean_value_2d_gausss))

print("\nCovariance matrix of 2D Gaussian:")
cov_mat_2d_gausss = np.cov(x1_2d_gauss, x2_2d_gauss)
print(cov_mat_2d_gausss)

#determ_cov_mat_2d_gausss = np.linalg.det(cov_mat_2d_gausss)
#print("\nDeterminant covariance matrix 2d Gaussian: " + str(determ_cov_mat_2d_gausss) + "\n")

#inverted_final_2d_gausss = np.linalg.inv(cov_mat_2d_gausss)
#print(inverted_final_2d_gausss)

from scipy.stats import multivariate_normal

print("\nUpper fraction of 2D Gaussian:")
multivariate_normal.pdf([100, 225], mean=mean_value_2d_gausss, cov=cov_mat_2d_gausss)

#RESULT WAS ALREADY MULTIPLIED BY P(Class)

print("For Class C=1\n")

x1_2d_gauss = [80,90,70]
x2_2d_gauss = [220,200,190]

mean_value_2d_gausss = np.mean([x1_2d_gauss, x2_2d_gauss], axis=1)
print("mean value for 2D Gaussian: " + str(mean_value_2d_gausss))

print("\nCovariance matrix of 2D Gaussian:")
cov_mat_2d_gausss = np.cov(x1_2d_gauss, x2_2d_gauss)
print(cov_mat_2d_gausss)

#determ_cov_mat_2d_gausss = np.linalg.det(cov_mat_2d_gausss)
#print("\nDeterminant covariance matrix 2d Gaussian: " + str(determ_cov_mat_2d_gausss) + "\n")

#inverted_final_2d_gausss = np.linalg.inv(cov_mat_2d_gausss)
#print(inverted_final_2d_gausss)

from scipy.stats import multivariate_normal

print("\nUpper fraction of 2D Gaussian:")
multivariate_normal.pdf([100, 225], mean=mean_value_2d_gausss, cov=cov_mat_2d_gausss)

#RESULT WAS ALREADY MULTIPLIED BY P(Class)

"""###PB nº 3 - Linear regression

####(X^T.X)^(-1) . X^T . T
"""

print("FORMULA:\n")
print("(X^T.X)^(-1) . X^T . T\n")

def feature_transf(x):
  #return np.log(x)
  return x
  #return x

linreg_input_matrix = [[1,feature_transf(2),feature_transf(4)],
                       [1,feature_transf(4),feature_transf(2)],
                       [1,feature_transf(5),feature_transf(6)],
                       [1,feature_transf(7),feature_transf(5)]]


linreg_target_matrix = [1,1.5,2,2.5]

print("original:")
print(linreg_input_matrix)

print("\ntranspose: \n")
print(np.transpose(linreg_input_matrix))

aux = np.dot(np.transpose(linreg_input_matrix), linreg_input_matrix)

print("\ntranspose * original: \n")
print(aux)

aux_ridge = [[4+4, 18, 17],
             [18, 94+4, 81],
             [17, 81, 81+4]]

print("\nplus 4I:\n")
print(aux_ridge)

linreg_first_part = np.linalg.inv(aux_ridge)

print("\n inverted plus 4I:\n")
print(linreg_first_part)

aux2 = np.dot(linreg_first_part, np.transpose(linreg_input_matrix))

print("\nFirst part * transposed:\n")
print(aux2)

linreg_second_part = np.dot(aux2, linreg_target_matrix)

print("\nFinal calculation:\n")
print(linreg_second_part)

"""####Predict target value for a certain x"""

#Predict target value for a certain x
#First add bias to X, then multiply by weights
#output(x) = w.x

added_bias_input_matrix = [1,1]

print("Result:")
np.dot(linreg_second_part, added_bias_input_matrix)

"""####Calculate mean squared error"""

#Calculate mean squared error
#(t - output(x))^2

MSE_vector = [0,0,0,0] #len = number of inputs
MSE_sum = 0

for i in range(0,len(MSE_vector)):
  print("target: " + str(linreg_target_matrix[i]))
  print("input: " + str(linreg_input_matrix[i]))
  aux3 = np.dot(linreg_second_part, linreg_input_matrix[i])
  print("aux3: " + str(aux3))
  MSE_vector[i] = (linreg_target_matrix[i] - aux3)**2
  print("(target - aux3)^2 = " + str(MSE_vector[i]))
  print("\n")
  MSE_sum+=MSE_vector[i]
  
print("final value MSE: " + str(MSE_sum/len(MSE_vector)))

"""###Stochastic GD?

### PB nº 4 - Forward Propagation
"""

from scipy.stats import logistic

#sigmoid
#logistic.cdf(#number)

learning_rate = 1

weights_1_forwprop = [[0.1, 0.1, 0.1, 0.1, 0.1],
                      [0.1, 0.1, 0.1, 0.1, 0.1],
                      [0.1, 0.1, 0.1, 0.1, 0.1]]
             
weights_2_forwprop = [[0.1, 0.1, 0.1],
                      [0.1, 0.1, 0.1]]

x0_forwprop = [1,1,0,0,0]

bias_1_forwprop = [0,0,0]
bias_2_forwprop = [0,0]

#x0
#z1 = w1*x0 + b1

#x1 = func(z1)
#z2 = w2*x1 + b2

#x2 = func(z2)

z1_forwprop = np.dot(weights_1_forwprop, x0_forwprop) + bias_1_forwprop
print("z1: ")
print(z1_forwprop)

print("\nx1: ")
x1_forwprop = np.transpose(logistic.cdf(z1_forwprop))
print(x1_forwprop)

z2_forwprop = np.dot(weights_2_forwprop, x1_forwprop)# + bias_2_forwprop
print("\nz2: ")
print(z2_forwprop)

x2_forwprop = logistic.cdf(z2_forwprop)
print("\nx2: ")
print(x2_forwprop)

[[-4589/10000], [5411/10000]]

"""###PB nº 5 - K-Means"""

centroids_kmeans = [[1,9], [2/3,3],[0,-10]]

input_kmeans = [[1,9],
                [0,8],
                [1,0],
                [1,1],
                [0,-10]]

for uu in input_kmeans:
  for yy in centroids_kmeans:
    print("\ninput: {}".format(uu))
    print("centroid: {}".format(yy))
    print( (spatial.distance.euclidean(uu, yy))**2 )

#see what is the centroid with highest number for each x, the highest number is
#the centroid that gets the x

#then see what points each centroid has, and calculate mean between old and new

new_centroids_kmeans = [0,0,0]
#points in each centroid by hand!!
new_centroids_kmeans[0] = np.mean([[1,9],[0,8]],axis=0)
new_centroids_kmeans[1] = [1,0.5] #np.mean([[0,2],[2,2]],axis=0)
new_centroids_kmeans[2] = [0,-10] #np.mean([[0,2],[2,2]],axis=0)
print("\nnew centroids:")
print(new_centroids_kmeans)

print("\ncalculating new centroids:\n")

print("distance between old centroid and new (if not 0, needs another epoch):")
print(spatial.distance.euclidean(centroids_kmeans[0], new_centroids_kmeans[0])**2)
print(spatial.distance.euclidean(centroids_kmeans[1], new_centroids_kmeans[1])**2)
print(spatial.distance.euclidean(centroids_kmeans[2], new_centroids_kmeans[2])**2)

# import numpy
from numpy import vstack,array
 
# scipy
from scipy.cluster.vq import kmeans,vq,whiten
 
data = input_kmeans

#plt.plot(data[:,0],data[:,1],'go')
#plt.show() 

# whiten the features
#data = whiten(data)
 
# find 3 clusters in the data
centroids,distortion = kmeans(data,2)
 
print('centroids  : ',centroids)

"""###PB nº 8 - PCA

####Calculate mean, instructions for cov matrix
"""

#K-L transformation

x_input_pca = [[0,0],[4,0],[2,1],[6,3]]

mean_pca = np.mean([[0,4,2,6], [0,0,1,3]], axis=1)
print("PCA mean vector:")
print(mean_pca)
print("\n")

#COV MATRIX:
#contributions:
#
#(ponto-mean)(ponto-mean).T = a
#(ponto-mean)(ponto-mean).T = b
#(ponto-mean)(ponto-mean).T = c
#(ponto-mean)(ponto-mean).T = d
#cov_mat = 1/(4-1) * (a+b+c+d)

"""x = np.array([
    [0, 0],
    [4, 0],
    [2, 1],
    [6, 3],
])


def calc_cov(x):
    m = [sum(x[0]) / len(x[0]), sum(x[1]) / len(x[1])]
    c = [[0, 0], [0, 0]]
    N = len(x.T)
    for i in range(2):
        for j in range(2):
            print("c[{}][{}] = {}".format(i, j, sum([(x[i][k] - m[i]) * (x[j][k] - m[j])
                                                     for k in range(N)]) / (N - 1)))
            c[i][j] = sum([(x[i][k] - m[i]) * (x[j][k] - m[j]) for k in range(N)]) / (N - 1)
    return c


def eigen_values(c):
    c = np.array(c)
    return np.linalg.eig(c)[0]


def eigen_vectors(c):
    c = np.array(c)
    return np.linalg.eig(c)[1]


def norm(x):
    return np.sqrt(sum([x[i] ** 2 for i in range(len(x))]))


def eigen_vector(c, l):
    c = np.array(c) - l * np.eye(2)
    u1 = -c[0][0] / c[0][1]
    print("u({}) = [1, {}]".format(l, u1))
    u = np.array([1, u1])
    u = u / norm(u)
    return u


c = calc_cov(x.T)
eigenvalues = eigen_values(c)
eigenvectors = eigen_vectors(c)
print("values = {}".format(eigenvalues))
print("vector({}) = {}".format(eigenvalues[0], eigen_vector(c, eigenvalues[0])))
print("vector({}) = {}".format(eigenvalues[1], eigen_vector(c, eigenvalues[1])))
# print("vectors =\n {}".format(eigenvectors))
print("transformed_date =\n {}".format(x.dot(eigen_vector(c, max(eigenvalues[0], eigenvalues[1])))))

####Map points into most significant dimension
"""

K_L_transform = [[0.4138, 0.9120],[-0.9106, 0.4104]]

x_new_transform = [0,0,0,0]

for p in range(0, len(x_input_pca)):
  x_new_transform[p] = np.dot(np.transpose(K_L_transform),x_input_pca[p])
  print(x_new_transform[p])

"""###CNN?

###RNN?

###Bayesian Networks?
"""